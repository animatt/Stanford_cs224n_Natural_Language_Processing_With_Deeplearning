(base) bash-3.2$ pwd
/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/features
(base) bash-3.2$ conda activate a2
(a2) bash-3.2$ behave
Exception ModuleNotFoundError: No module named 'word2vec'
Traceback (most recent call last):
  File "/usr/local/anaconda3/envs/a2/bin/behave", line 10, in <module>
    sys.exit(main())
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/__main__.py", line 183, in main
    return run_behave(config)
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/__main__.py", line 127, in run_behave
    failed = runner.run()
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner.py", line 804, in run
    return self.run_with_paths()
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner.py", line 809, in run_with_paths
    self.load_step_definitions()
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner.py", line 796, in load_step_definitions
    load_step_modules(step_paths)
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner_util.py", line 412, in load_step_modules
    exec_file(os.path.join(path, name), step_module_globals)
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner_util.py", line 386, in exec_file
    exec(code, globals_, locals_)
  File "steps/word2vec_step.py", line 4, in <module>
    from word2vec import sigmoid
ModuleNotFoundError: No module named 'word2vec'
(a2) bash-3.2$ pwd
/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/features
(a2) bash-3.2$ cd ..
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.001s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                         # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                   # None
    Given a naiveSoftmaxLossAndGradient function                                                                                                                   # None
    When it is applied to center_word_vec: <center_word_vec:NdArray>, outside_word_idx: <outside_word_idx:NdArray>, outside_word_vecs: <outside_word_vecs:NdArray> # None
    Then it returns loss: <loss:f>, grad_center_vec: <grad_center_vec:NdArray>, grad_outside_vecs: <grad_outside_vecs:NdArray>                                     # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
12 steps passed, 0 failed, 0 skipped, 3 undefined
Took 0m0.003s

You can implement step definitions for undefined steps with these snippets:

@given(u'a naiveSoftmaxLossAndGradient function')
def step_impl(context):
    raise NotImplementedError(u'STEP: Given a naiveSoftmaxLossAndGradient function')


@when(u'it is applied to center_word_vec: <center_word_vec:NdArray>, outside_word_idx: <outside_word_idx:NdArray>, outside_word_vecs: <outside_word_vecs:NdArray>')
def step_impl(context):
    raise NotImplementedError(u'STEP: When it is applied to center_word_vec: <center_word_vec:NdArray>, outside_word_idx: <outside_word_idx:NdArray>, outside_word_vecs: <outside_word_vecs:NdArray>')


@then(u'it returns loss: <loss:f>, grad_center_vec: <grad_center_vec:NdArray>, grad_outside_vecs: <grad_outside_vecs:NdArray>')
def step_impl(context):
    raise NotImplementedError(u'STEP: Then it returns loss: <loss:f>, grad_center_vec: <grad_center_vec:NdArray>, grad_outside_vecs: <grad_outside_vecs:NdArray>')

(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # None
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # None
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_word_vecs: <outside_word_vecs>                                                                                     # None
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
12 steps passed, 0 failed, 1 skipped, 2 undefined
Took 0m0.002s

You can implement step definitions for undefined steps with these snippets:

@given(u'a naiveSoftmaxLossAndGradient function')
def step_impl(context):
    raise NotImplementedError(u'STEP: Given a naiveSoftmaxLossAndGradient function')


@when(u'it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_word_vecs: <outside_word_vecs>')
def step_impl(context):
    raise NotImplementedError(u'STEP: When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_word_vecs: <outside_word_vecs>')

(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.002s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
      TypeError: step_imp() missing 1 required positional argument: 'out'

    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_word_vecs: <outside_word_vecs>                                                                                     # None
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
12 steps passed, 1 failed, 1 skipped, 1 undefined
Took 0m0.004s

You can implement step definitions for undefined steps with these snippets:

@when(u'it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_word_vecs: <outside_word_vecs>')
def step_impl(context):
    raise NotImplementedError(u'STEP: When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_word_vecs: <outside_word_vecs>')

(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
      TypeError: step_imp() missing 1 required positional argument: 'out'

    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: <outside_vectorss>                                                                                        # None
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
12 steps passed, 1 failed, 1 skipped, 1 undefined
Took 0m0.003s

You can implement step definitions for undefined steps with these snippets:

@when(u'it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: <outside_vectorss>')
def step_impl(context):
    raise NotImplementedError(u'STEP: When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: <outside_vectorss>')

(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
      TypeError: step_imp() missing 1 required positional argument: 'out'

    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: <outside_vectors>                                                                                         # None
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
12 steps passed, 1 failed, 1 skipped, 1 undefined
Took 0m0.003s

You can implement step definitions for undefined steps with these snippets:

@when(u'it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: <outside_vectors>')
def step_impl(context):
    raise NotImplementedError(u'STEP: When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: <outside_vectors>')

(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.000s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.001s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
      TypeError: step_imp() missing 1 required positional argument: 'out'

    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # None
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
12 steps passed, 1 failed, 1 skipped, 1 undefined
Took 0m0.003s

You can implement step definitions for undefined steps with these snippets:

@when(u'it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]')
def step_impl(context):
    raise NotImplementedError(u'STEP: When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]')

(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
      TypeError: step_imp() missing 1 required positional argument: 'out'

    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # None
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
12 steps passed, 1 failed, 1 skipped, 1 undefined
Took 0m0.003s

You can implement step definitions for undefined steps with these snippets:

@when(u'it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]')
def step_impl(context):
    raise NotImplementedError(u'STEP: When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]')

(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
      TypeError: step_imp() missing 1 required positional argument: 'out'

    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # None
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
12 steps passed, 1 failed, 1 skipped, 1 undefined
Took 0m0.003s

You can implement step definitions for undefined steps with these snippets:

@when(u'it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]')
def step_impl(context):
    raise NotImplementedError(u'STEP: When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]')

(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
      TypeError: step_imp() missing 1 required positional argument: 'out'

    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # None
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
12 steps passed, 1 failed, 2 skipped, 0 undefined
Took 0m0.003s
(a2) bash-3.2$ python --version
Python 3.7.4
(a2) bash-3.2$ python
Python 3.7.4 (default, Aug  9 2019, 12:36:10) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import numpy as np
>>> np.testing.assert_allclose(3, 3)
>>> np.testing.assert_allclose(3, 32)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/numpy/testing/_private/utils.py", line 1501, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/numpy/testing/_private/utils.py", line 827, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0

Mismatch: 100%
Max absolute difference: 29
Max relative difference: 0.90625
 x: array(3)
 y: array(32)
>>> np.testing.assert_allclose(3, 3)
>>> 
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 0.002s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
        File "features/steps/word2vec_step.py", line 41, in step_imp
          None
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 64, in naiveSoftmaxLossAndGradient
          return loss, gradCenterVec, gradOutsideVecs
      NameError: name 'loss' is not defined

    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
13 steps passed, 1 failed, 1 skipped, 0 undefined
Took 0m0.004s
(a2) bash-3.2$ python
Python 3.7.4 (default, Aug  9 2019, 12:36:10) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import numpy as np
>>> 5.exp
  File "<stdin>", line 1
    5.exp
        ^
SyntaxError: invalid syntax
>>> np.arange(5).exp
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'numpy.ndarray' object has no attribute 'exp'
>>> np.arange(5).sum
<built-in method sum of numpy.ndarray object at 0x105f0d800>
>>> np.arange(5).sum()
10
>>> np.arange(5).exp()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'numpy.ndarray' object has no attribute 'exp'
>>> 
(a2) bash-3.2$ python
Python 3.7.4 (default, Aug  9 2019, 12:36:10) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import numpy as np
>>> vec  = np.arange(5)[:, None]
>>> pp vec
  File "<stdin>", line 1
    pp vec
         ^
SyntaxError: invalid syntax
>>> print(vec)
[[0]
 [1]
 [2]
 [3]
 [4]]
>>> print(vec == 2)
[[False]
 [False]
 [ True]
 [False]
 [False]]
>>> print((vec == 2) + (vec == 1))
[[False]
 [ True]
 [ True]
 [False]
 [False]]
>>> print((vec == 2) + (np.arange(5)[:, None]))
[[0]
 [1]
 [3]
 [3]
 [4]]
>>> vec = np.arange(5)
>>> print(vec)
[0 1 2 3 4]
>>> Mat = np.arange(25).reshape((5, 5))
>>> print(Mat)
[[ 0  1  2  3  4]
 [ 5  6  7  8  9]
 [10 11 12 13 14]
 [15 16 17 18 19]
 [20 21 22 23 24]]
>>> print(Mat @ vec)
[ 30  80 130 180 230]
>>> print(vec @ Mat)
[150 160 170 180 190]
>>> 
(a2) bash-3.2$ behave
Exception SyntaxError: invalid syntax (word2vec.py, line 74)
Traceback (most recent call last):
  File "/usr/local/anaconda3/envs/a2/bin/behave", line 10, in <module>
    sys.exit(main())
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/__main__.py", line 183, in main
    return run_behave(config)
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/__main__.py", line 127, in run_behave
    failed = runner.run()
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner.py", line 804, in run
    return self.run_with_paths()
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner.py", line 809, in run_with_paths
    self.load_step_definitions()
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner.py", line 796, in load_step_definitions
    load_step_modules(step_paths)
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner_util.py", line 412, in load_step_modules
    exec_file(os.path.join(path, name), step_module_globals)
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner_util.py", line 386, in exec_file
    exec(code, globals_, locals_)
  File "features/steps/word2vec_step.py", line 4, in <module>
    from word2vec import sigmoid, naiveSoftmaxLossAndGradient
  File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 74
    return loss, gradCenterVec, gradOutsideVecs
         ^
SyntaxError: invalid syntax
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 0.001s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
        File "features/steps/word2vec_step.py", line 41, in step_imp
          None
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 57, in naiveSoftmaxLossAndGradient
          N, _ = outsideVectors.size
      TypeError: cannot unpack non-iterable int object

    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
13 steps passed, 1 failed, 1 skipped, 0 undefined
Took 0m0.003s
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 0.000s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
        File "features/steps/word2vec_step.py", line 41, in step_imp
          None
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 57, in naiveSoftmaxLossAndGradient
          N, _ = outsideVectors.size
      TypeError: cannot unpack non-iterable int object

    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
13 steps passed, 1 failed, 1 skipped, 0 undefined
Took 0m0.002s
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32

print("what")

  C-c C-c
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 13.504s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
        File "features/steps/word2vec_step.py", line 41, in step_imp
          None
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 58, in naiveSoftmaxLossAndGradient
          N = outsideVectors.size
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 58, in naiveSoftmaxLossAndGradient
          N = outsideVectors.size
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 88, in trace_dispatch
          return self.dispatch_line(frame)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 113, in dispatch_line
          if self.quitting: raise BdbQuit
      bdb.BdbQuit
      
      Captured stdout:
      > /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(58)naiveSoftmaxLossAndGradient()
      -> N = outsideVectors.size
      (Pdb) (Pdb) what
      (Pdb) what
      (Pdb) --KeyboardInterrupt--
      (Pdb) what
      (Pdb)

    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
13 steps passed, 1 failed, 1 skipped, 0 undefined
Took 0m13.506s
(a2) bash-3.2$ python
Python 3.7.4 (default, Aug  9 2019, 12:36:10) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> from  word2vec import *
>>> 
(a2) bash-3.2$ pwd
/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2
(a2) bash-3.2$ python
Python 3.7.4 (default, Aug  9 2019, 12:36:10) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> from  word2vec import *
>>> naiveSoftmaxLossAndGradient(1, 1, 1, 1)
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(58)naiveSoftmaxLossAndGradient()
-> N = outsideVectors.size
(Pdb) 
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 58, in naiveSoftmaxLossAndGradient
    N = outsideVectors.size
  File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 58, in naiveSoftmaxLossAndGradient
    N = outsideVectors.size
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
>>> 
(a2) bash-3.2$ behave --no-capture
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(58)naiveSoftmaxLossAndGradient()
-> N = outsideVectors.size
(Pdb) outsideVectors.size
9
(Pdb) outsideVectors
array([[1., 2., 3.],
       [4., 5., 6.],
       [7., 8., 9.]])
(Pdb) outsideVectors.shape
(3, 3)
(Pdb) 
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 620.802s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
        File "features/steps/word2vec_step.py", line 41, in step_imp
          None
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 58, in naiveSoftmaxLossAndGradient
          N, _ = outsideVectors.shape
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 58, in naiveSoftmaxLossAndGradient
          N, _ = outsideVectors.shape
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 88, in trace_dispatch
          return self.dispatch_line(frame)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 113, in dispatch_line
          if self.quitting: raise BdbQuit
      bdb.BdbQuit

    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
13 steps passed, 1 failed, 1 skipped, 0 undefined
Took 10m20.805s
(a2) bash-3.2$ behave --no-capture
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(58)naiveSoftmaxLossAndGradient()
-> N, _ = outsideVectors.shape
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(60)naiveSoftmaxLossAndGradient()
-> outside_vec_scores = np.exp(outsideVectors.T @ centerWordVec)
(Pdb) l
 55  	    ### YOUR CODE HERE
 56  	
 57  	    breakpoint()
 58  	    N, _ = outsideVectors.shape
 59  	
 60  ->	    outside_vec_scores = np.exp(outsideVectors.T @ centerWordVec)
 61  	    score_normalizer = outside_vec_scores.sum(axis=0)
 62  	    outside_vec_probs = outside_vec_scores / outside_vec_probs
 63  	
 64  	    loss = - np.log(outside_vec_probs[outsideWordIdx])
 65  	    gradCenterVec = outsideVectors @ (
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(61)naiveSoftmaxLossAndGradient()
-> score_normalizer = outside_vec_scores.sum(axis=0)
(Pdb) outside_vec_scores
array([ 2208.34799189, 12088.38073022, 66171.16016838])
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(62)naiveSoftmaxLossAndGradient()
-> outside_vec_probs = outside_vec_scores / outside_vec_probs
(Pdb) l
 57  	    breakpoint()
 58  	    N, _ = outsideVectors.shape
 59  	
 60  	    outside_vec_scores = np.exp(outsideVectors.T @ centerWordVec)
 61  	    score_normalizer = outside_vec_scores.sum(axis=0)
 62  ->	    outside_vec_probs = outside_vec_scores / outside_vec_probs
 63  	
 64  	    loss = - np.log(outside_vec_probs[outsideWordIdx])
 65  	    gradCenterVec = outsideVectors @ (
 66  	        outside_vec_probs - (np.arange(N)[:, None] == outsideWordIdx)
 67  	    )
(Pdb) score_normalizer
80467.88889048077
(Pdb) n
UnboundLocalError: local variable 'outside_vec_probs' referenced before assignment
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(62)naiveSoftmaxLossAndGradient()
-> outside_vec_probs = outside_vec_scores / outside_vec_probs
(Pdb) outsid_vec_probls
*** NameError: name 'outsid_vec_probls' is not defined
(Pdb) outsid_vec_probs
*** NameError: name 'outsid_vec_probs' is not defined
(Pdb) outside_vec_probs
*** NameError: name 'outside_vec_probs' is not defined
(Pdb) l
 57  	    breakpoint()
 58  	    N, _ = outsideVectors.shape
 59  	
 60  	    outside_vec_scores = np.exp(outsideVectors.T @ centerWordVec)
 61  	    score_normalizer = outside_vec_scores.sum(axis=0)
 62  ->	    outside_vec_probs = outside_vec_scores / outside_vec_probs
 63  	
 64  	    loss = - np.log(outside_vec_probs[outsideWordIdx])
 65  	    gradCenterVec = outsideVectors @ (
 66  	        outside_vec_probs - (np.arange(N)[:, None] == outsideWordIdx)
 67  	    )
(Pdb) n
--Return--
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(62)naiveSoftmaxLossAndGradient()->None
-> outside_vec_probs = outside_vec_scores / outside_vec_probs
(Pdb) n
UnboundLocalError: local variable 'outside_vec_probs' referenced before assignment
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/features/steps/word2vec_step.py(41)step_imp()
-> None
(Pdb) outside_vec_probs
*** NameError: name 'outside_vec_probs' is not defined
(Pdb) outside_vec_probs
*** NameError: name 'outside_vec_probs' is not defined
(Pdb) l
 36  	             outside_vectors):
 37  	    context.result = naiveSoftmaxLossAndGradient(
 38  	        center_word_vec,
 39  	        outside_word_idx,
 40  	        outside_vectors,
 41  ->	        None
 42  	    )
 43  	
 44  	@then('it returns loss: {loss:f}, grad_center_vec: {grad_center_vec:NdArray}, grad_outside_vecs: {grad_outside_vecs:NdArray}')
 45  	def step_imp(context,
 46  	             loss,
(Pdb) n
--Return--
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/features/steps/word2vec_step.py(41)step_imp()->None
-> None
(Pdb) l
 36  	             outside_vectors):
 37  	    context.result = naiveSoftmaxLossAndGradient(
 38  	        center_word_vec,
 39  	        outside_word_idx,
 40  	        outside_vectors,
 41  ->	        None
 42  	    )
 43  	
 44  	@then('it returns loss: {loss:f}, grad_center_vec: {grad_center_vec:NdArray}, grad_outside_vecs: {grad_outside_vecs:NdArray}')
 45  	def step_imp(context,
 46  	             loss,
(Pdb) n
UnboundLocalError: local variable 'outside_vec_probs' referenced before assignment
> /usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py(98)run()
-> self.func(context, *args, **kwargs)
(Pdb) l
 93  	                kwargs[arg.name] = arg.value
 94  	            else:
 95  	                args.append(arg.value)
 96  	
 97  	        with context.use_with_user_mode():
 98  ->	            self.func(context, *args, **kwargs)
 99  	
100  	    @staticmethod
101  	    def make_location(step_function):
102  	        """Extracts the location information from the step function and
103  	        builds a FileLocation object with (filename, line_number) info.
(Pdb) 
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 1924.544s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 94, in trace_dispatch
          return self.dispatch_exception(frame, arg)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 174, in dispatch_exception
          if self.quitting: raise BdbQuit
      bdb.BdbQuit

    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
13 steps passed, 1 failed, 1 skipped, 0 undefined
Took 32m4.547s
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
  C-c C-c
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 3.394s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
        File "features/steps/word2vec_step.py", line 41, in step_imp
          None
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 58, in naiveSoftmaxLossAndGradient
          N, _ = outsideVectors.shape
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 58, in naiveSoftmaxLossAndGradient
          N, _ = outsideVectors.shape
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 88, in trace_dispatch
          return self.dispatch_line(frame)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 113, in dispatch_line
          if self.quitting: raise BdbQuit
      bdb.BdbQuit
      
      Captured stdout:
      > /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(58)naiveSoftmaxLossAndGradient()
      -> N, _ = outsideVectors.shape
      (Pdb) --KeyboardInterrupt--
      (Pdb) (Pdb)

    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
13 steps passed, 1 failed, 1 skipped, 0 undefined
Took 0m3.397s
(a2) bash-3.2$ behave --no-capture
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(58)naiveSoftmaxLossAndGradient()
-> N, _ = outsideVectors.shape
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(60)naiveSoftmaxLossAndGradient()
-> outside_vec_scores = np.exp(outsideVectors.T @ centerWordVec)
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(61)naiveSoftmaxLossAndGradient()
-> score_normalizer = outside_vec_scores.sum(axis=0)
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(62)naiveSoftmaxLossAndGradient()
-> outside_vec_probs = outside_vec_scores / outside_vec_probs
(Pdb) n
UnboundLocalError: local variable 'outside_vec_probs' referenced before assignment
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(62)naiveSoftmaxLossAndGradient()
-> outside_vec_probs = outside_vec_scores / outside_vec_probs
(Pdb) 
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 17.884s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
        File "features/steps/word2vec_step.py", line 41, in step_imp
          None
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 62, in naiveSoftmaxLossAndGradient
          outside_vec_probs = outside_vec_scores / outside_vec_probs
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 94, in trace_dispatch
          return self.dispatch_exception(frame, arg)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 174, in dispatch_exception
          if self.quitting: raise BdbQuit
      bdb.BdbQuit

    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
13 steps passed, 1 failed, 1 skipped, 0 undefined
Took 0m17.886s
(a2) bash-3.2$ behave --no-capture
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.000s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(58)naiveSoftmaxLossAndGradient()
-> N, _ = outsideVectors.shape
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(60)naiveSoftmaxLossAndGradient()
-> outside_vec_scores = np.exp(outsideVectors.T @ centerWordVec)
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(61)naiveSoftmaxLossAndGradient()
-> score_normalizer = outside_vec_scores.sum(axis=0)
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(62)naiveSoftmaxLossAndGradient()
-> outside_vec_probs = outside_vec_scores / score_normalizer
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(64)naiveSoftmaxLossAndGradient()
-> loss = - np.log(outside_vec_probs[outsideWordIdx])
(Pdb) outside_vec_probs
array([0.02744384, 0.15022614, 0.82233001])
(Pdb) outside_vec_probs.sum()
1.0
(Pdb) l
 59  	
 60  	    outside_vec_scores = np.exp(outsideVectors.T @ centerWordVec)
 61  	    score_normalizer = outside_vec_scores.sum(axis=0)
 62  	    outside_vec_probs = outside_vec_scores / score_normalizer
 63  	
 64  ->	    loss = - np.log(outside_vec_probs[outsideWordIdx])
 65  	    gradCenterVec = outsideVectors @ (
 66  	        outside_vec_probs - (np.arange(N)[:, None] == outsideWordIdx)
 67  	    )
 68  	    gradOutsideVecs = centerWordVec @ (2)
 69  	
(Pdb) outside_vec_probs[outsideWordIdx]
0.15022614482492067
(Pdb) l
 70  	
 71  	    ### END YOUR CODE
 72  	
 73  	
 74  	    return loss, gradCenterVec, gradOutsideVecs
 75  	
 76  	
 77  	def getNegativeSamples(outsideWordIdx, dataset, K):
 78  	    """ Samples K indexes which are not the outsideWordIdx """
 79  	
 80  	    negSampleWordIndices = [None] * K
(Pdb) l
 81  	    for k in range(K):
 82  	        newidx = dataset.sampleTokenIdx()
 83  	        while newidx == outsideWordIdx:
 84  	            newidx = dataset.sampleTokenIdx()
 85  	        negSampleWordIndices[k] = newidx
 86  	    return negSampleWordIndices
 87  	
 88  	
 89  	def negSamplingLossAndGradient(
 90  	    centerWordVec,
 91  	    outsideWordIdx,
(Pdb) l
 92  	    outsideVectors,
 93  	    dataset,
 94  	    K=10
 95  	):
 96  	    """ Negative sampling loss function for word2vec models
 97  	
 98  	    Implement the negative sampling loss and gradients for a centerWordVec
 99  	    and a outsideWordIdx word vector as a building block for word2vec
100  	    models. K is the number of negative samples to take.
101  	
102  	    Note: The same word may be negatively sampled multiple times. For
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(65)naiveSoftmaxLossAndGradient()
-> gradCenterVec = outsideVectors @ (
(Pdb) l
 60  	    outside_vec_scores = np.exp(outsideVectors.T @ centerWordVec)
 61  	    score_normalizer = outside_vec_scores.sum(axis=0)
 62  	    outside_vec_probs = outside_vec_scores / score_normalizer
 63  	
 64  	    loss = - np.log(outside_vec_probs[outsideWordIdx])
 65  ->	    gradCenterVec = outsideVectors @ (
 66  	        outside_vec_probs - (np.arange(N)[:, None] == outsideWordIdx)
 67  	    )
 68  	    gradOutsideVecs = centerWordVec @ (2)
 69  	
 70  	
(Pdb) l
 71  	    ### END YOUR CODE
 72  	
 73  	
 74  	    return loss, gradCenterVec, gradOutsideVecs
 75  	
 76  	
 77  	def getNegativeSamples(outsideWordIdx, dataset, K):
 78  	    """ Samples K indexes which are not the outsideWordIdx """
 79  	
 80  	    negSampleWordIndices = [None] * K
 81  	    for k in range(K):
(Pdb) ll
 26  	def naiveSoftmaxLossAndGradient(
 27  	    centerWordVec,
 28  	    outsideWordIdx,
 29  	    outsideVectors,
 30  	    dataset
 31  	):
 32  	    """ Naive Softmax loss & gradient function for word2vec models
 33  	
 34  	    Implement the naive softmax loss and gradients between a center word's
 35  	    embedding and an outside word's embedding. This will be the building block
 36  	    for our word2vec models.
 37  	
 38  	    Arguments:
 39  	    centerWordVec -- numpy ndarray, center word's embedding
 40  	                    (v_c in the pdf handout)
 41  	    outsideWordIdx -- integer, the index of the outside word
 42  	                    (o of u_o in the pdf handout)
 43  	    outsideVectors -- outside vectors (rows of matrix) for all words in vocab
 44  	                      (U in the pdf handout)
 45  	    dataset -- needed for negative sampling, unused here.
 46  	
 47  	    Return:
 48  	    loss -- naive softmax loss
 49  	    gradCenterVec -- the gradient with respect to the center word vector
 50  	                     (dJ / dv_c in the pdf handout)
 51  	    gradOutsideVecs -- the gradient with respect to all the outside word vectors
 52  	                    (dJ / dU)
 53  	    """
 54  	
 55  	    ### YOUR CODE HERE
 56  	
 57  	    breakpoint()
 58  	    N, _ = outsideVectors.shape
 59  	
 60  	    outside_vec_scores = np.exp(outsideVectors.T @ centerWordVec)
 61  	    score_normalizer = outside_vec_scores.sum(axis=0)
 62  	    outside_vec_probs = outside_vec_scores / score_normalizer
 63  	
 64  	    loss = - np.log(outside_vec_probs[outsideWordIdx])
 65  ->	    gradCenterVec = outsideVectors @ (
 66  	        outside_vec_probs - (np.arange(N)[:, None] == outsideWordIdx)
 67  	    )
 68  	    gradOutsideVecs = centerWordVec @ (2)
 69  	
 70  	
 71  	    ### END YOUR CODE
 72  	
 73  	
 74  	    return loss, gradCenterVec, gradOutsideVecs
(Pdb) loss
1.895613488056148
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(66)naiveSoftmaxLossAndGradient()
-> outside_vec_probs - (np.arange(N)[:, None] == outsideWordIdx)
(Pdb) l
 61  	    score_normalizer = outside_vec_scores.sum(axis=0)
 62  	    outside_vec_probs = outside_vec_scores / score_normalizer
 63  	
 64  	    loss = - np.log(outside_vec_probs[outsideWordIdx])
 65  	    gradCenterVec = outsideVectors @ (
 66  ->	        outside_vec_probs - (np.arange(N)[:, None] == outsideWordIdx)
 67  	    )
 68  	    gradOutsideVecs = centerWordVec @ (2)
 69  	
 70  	
 71  	    ### END YOUR CODE
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(68)naiveSoftmaxLossAndGradient()
-> gradOutsideVecs = centerWordVec @ (2)
(Pdb) n
ValueError: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(68)naiveSoftmaxLossAndGradient()
-> gradOutsideVecs = centerWordVec @ (2)
(Pdb) l
 63  	
 64  	    loss = - np.log(outside_vec_probs[outsideWordIdx])
 65  	    gradCenterVec = outsideVectors @ (
 66  	        outside_vec_probs - (np.arange(N)[:, None] == outsideWordIdx)
 67  	    )
 68  ->	    gradOutsideVecs = centerWordVec @ (2)
 69  	
 70  	
 71  	    ### END YOUR CODE
 72  	
 73  	
(Pdb) ll
 26  	def naiveSoftmaxLossAndGradient(
 27  	    centerWordVec,
 28  	    outsideWordIdx,
 29  	    outsideVectors,
 30  	    dataset
 31  	):
 32  	    """ Naive Softmax loss & gradient function for word2vec models
 33  	
 34  	    Implement the naive softmax loss and gradients between a center word's
 35  	    embedding and an outside word's embedding. This will be the building block
 36  	    for our word2vec models.
 37  	
 38  	    Arguments:
 39  	    centerWordVec -- numpy ndarray, center word's embedding
 40  	                    (v_c in the pdf handout)
 41  	    outsideWordIdx -- integer, the index of the outside word
 42  	                    (o of u_o in the pdf handout)
 43  	    outsideVectors -- outside vectors (rows of matrix) for all words in vocab
 44  	                      (U in the pdf handout)
 45  	    dataset -- needed for negative sampling, unused here.
 46  	
 47  	    Return:
 48  	    loss -- naive softmax loss
 49  	    gradCenterVec -- the gradient with respect to the center word vector
 50  	                     (dJ / dv_c in the pdf handout)
 51  	    gradOutsideVecs -- the gradient with respect to all the outside word vectors
 52  	                    (dJ / dU)
 53  	    """
 54  	
 55  	    ### YOUR CODE HERE
 56  	
 57  	    breakpoint()
 58  	    N, _ = outsideVectors.shape
 59  	
 60  	    outside_vec_scores = np.exp(outsideVectors.T @ centerWordVec)
 61  	    score_normalizer = outside_vec_scores.sum(axis=0)
 62  	    outside_vec_probs = outside_vec_scores / score_normalizer
 63  	
 64  	    loss = - np.log(outside_vec_probs[outsideWordIdx])
 65  	    gradCenterVec = outsideVectors @ (
 66  	        outside_vec_probs - (np.arange(N)[:, None] == outsideWordIdx)
 67  	    )
 68  ->	    gradOutsideVecs = centerWordVec @ (2)
 69  	
 70  	
 71  	    ### END YOUR CODE
 72  	
 73  	
 74  	    return loss, gradCenterVec, gradOutsideVecs
(Pdb) gradCenterVec
array([[-1.83533695, -1.09864313,  2.93398008],
       [-4.58834238, -2.74660783,  7.3349502 ],
       [-7.3413478 , -4.39457252, 11.73592033]])
(Pdb) outsideVectors
array([[1., 2., 3.],
       [4., 5., 6.],
       [7., 8., 9.]])
(Pdb) outside_vec_probs
array([0.02744384, 0.15022614, 0.82233001])
(Pdb) 
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 372.460s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
        File "features/steps/word2vec_step.py", line 41, in step_imp
          None
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 68, in naiveSoftmaxLossAndGradient
          gradOutsideVecs = centerWordVec @ (2)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 94, in trace_dispatch
          return self.dispatch_exception(frame, arg)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 174, in dispatch_exception
          if self.quitting: raise BdbQuit
      bdb.BdbQuit

    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
13 steps passed, 1 failed, 1 skipped, 0 undefined
Took 6m12.462s
(a2) bash-3.2$ behave --no-capture
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(58)naiveSoftmaxLossAndGradient()
-> N, _ = outsideVectors.shape
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(60)naiveSoftmaxLossAndGradient()
-> outside_vec_scores = np.exp(outsideVectors.T @ centerWordVec)
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(61)naiveSoftmaxLossAndGradient()
-> score_normalizer = outside_vec_scores.sum(axis=0)
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(62)naiveSoftmaxLossAndGradient()
-> outside_vec_probs = outside_vec_scores / score_normalizer
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(64)naiveSoftmaxLossAndGradient()
-> loss = - np.log(outside_vec_probs[outsideWordIdx])
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(65)naiveSoftmaxLossAndGradient()
-> gradCenterVec = outsideVectors @ (
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(66)naiveSoftmaxLossAndGradient()
-> outside_vec_probs - (np.arange(N) == outsideWordIdx)
(Pdb) l
 61  	    score_normalizer = outside_vec_scores.sum(axis=0)
 62  	    outside_vec_probs = outside_vec_scores / score_normalizer
 63  	
 64  	    loss = - np.log(outside_vec_probs[outsideWordIdx])
 65  	    gradCenterVec = outsideVectors @ (
 66  ->	        outside_vec_probs - (np.arange(N) == outsideWordIdx)
 67  	    )
 68  	    gradOutsideVecs = centerWordVec @ (2)
 69  	
 70  	
 71  	    ### END YOUR CODE
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(68)naiveSoftmaxLossAndGradient()
-> gradOutsideVecs = centerWordVec @ (2)
(Pdb) l
 63  	
 64  	    loss = - np.log(outside_vec_probs[outsideWordIdx])
 65  	    gradCenterVec = outsideVectors @ (
 66  	        outside_vec_probs - (np.arange(N) == outsideWordIdx)
 67  	    )
 68  ->	    gradOutsideVecs = centerWordVec @ (2)
 69  	
 70  	
 71  	    ### END YOUR CODE
 72  	
 73  	
(Pdb) gradCenterVec
array([0.79488617, 0.79488617, 0.79488617])
(Pdb) 
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 1100.031s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
        File "features/steps/word2vec_step.py", line 41, in step_imp
          None
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 68, in naiveSoftmaxLossAndGradient
          gradOutsideVecs = centerWordVec @ (2)
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 68, in naiveSoftmaxLossAndGradient
          gradOutsideVecs = centerWordVec @ (2)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 88, in trace_dispatch
          return self.dispatch_line(frame)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/bdb.py", line 113, in dispatch_line
          if self.quitting: raise BdbQuit
      bdb.BdbQuit

    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
13 steps passed, 1 failed, 1 skipped, 0 undefined
Took 18m20.033s
(a2) bash-3.2$ python
Python 3.7.4 (default, Aug  9 2019, 12:36:10) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import numpy as np
>>> vec = np.arange(5)
>>> print(vec)
[0 1 2 3 4]
>>> vec = vec[:, None]
>>> print(vec)
[[0]
 [1]
 [2]
 [3]
 [4]]
>>> vec2 = np.arange(5)
>>> vec @ vec2
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 1)
>>> vec @ vec2[None, :]
array([[ 0,  0,  0,  0,  0],
       [ 0,  1,  2,  3,  4],
       [ 0,  2,  4,  6,  8],
       [ 0,  3,  6,  9, 12],
       [ 0,  4,  8, 12, 16]])
>>> 
(a2) bash-3.2$ python
Python 3.7.4 (default, Aug  9 2019, 12:36:10) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import numpy as np
>>> vec1 = np.arange(5)
>>> vec2 = np.arange(5)
>>> vec1 @ vec2
30
>>> 
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 0.001s
      Traceback (most recent call last):
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/model.py", line 1329, in run
          match.run(runner.context)
        File "/usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/matchers.py", line 98, in run
          self.func(context, *args, **kwargs)
        File "features/steps/word2vec_step.py", line 41, in step_imp
          None
        File "/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py", line 68, in naiveSoftmaxLossAndGradient
          outsid_vec_probs - (np.arange(N) == outsideWordIdx)
      NameError: name 'outsid_vec_probs' is not defined

    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # None


Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
13 steps passed, 1 failed, 1 skipped, 0 undefined
Took 0m0.003s
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 0.000s
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44 0.002s
      Assertion Failed: 
      Not equal to tolerance rtol=1e-07, atol=0
      
      (shapes (3, 3), (3,) mismatch)
       x: array([[ 0.005489, -0.169955,  0.164466],
             [ 0.027444, -0.849774,  0.82233 ],
             [ 0.013722, -0.424887,  0.411165]])
       y: array([0.794886, 0.794886, 0.794886])



Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
14 steps passed, 1 failed, 0 skipped, 0 undefined
Took 0m0.004s
(a2) bash-3.2$ behave --no-capture
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.000s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(68)naiveSoftmaxLossAndGradient()
-> gradOutsideVecs = centerWordVec[:, None] @ (
(Pdb)     gradOutsideVecs = centerWordVec[:, None] @ (
        outside_vec_probs - (np.arange(N) == outsideWordIdx)
    )[None, :]

*** SyntaxError: unexpected EOF while parsing
(Pdb) array([ 0.02744384, -0.84977386,  0.82233001])
(Pdb) *** SyntaxError: invalid syntax
(Pdb) *** SyntaxError: invalid syntax
(Pdb)  gradOutsideVecs = centerWordVec[:, None] @ (outside_vec_probs - (np.arange(N) == outsideWordIdx))[None, :]

(Pdb) (Pdb) gradOutsideVecs
array([[ 0.00548877, -0.16995477,  0.164466  ],
       [ 0.02744384, -0.84977386,  0.82233001],
       [ 0.01372192, -0.42488693,  0.41116501]])
(Pdb) 
array([[ 0.00548877, -0.16995477,  0.164466  ],
       [ 0.02744384, -0.84977386,  0.82233001],
       [ 0.01372192, -0.42488693,  0.41116501]])
(Pdb) gradCenterVec
array([0.79488617, 0.79488617, 0.79488617])
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(69)naiveSoftmaxLossAndGradient()
-> outside_vec_probs - (np.arange(N) == outsideWordIdx)
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(70)naiveSoftmaxLossAndGradient()
-> )[None, :]
(Pdb) n
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(76)naiveSoftmaxLossAndGradient()
-> return loss, gradCenterVec, gradOutsideVecs
(Pdb) n
--Return--
> /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2/word2vec.py(76)naiveSoftmaxLossAndGradient()->(1.895613488056148, array([0.7948..., 0.79488617]), array([[ 0.00... 0.41116501]]))
-> return loss, gradCenterVec, gradOutsideVecs
(Pdb) l
 71  	
 72  	
 73  	    ### END YOUR CODE
 74  	
 75  	
 76  ->	    return loss, gradCenterVec, gradOutsideVecs
 77  	
 78  	
 79  	def getNegativeSamples(outsideWordIdx, dataset, K):
 80  	    """ Samples K indexes which are not the outsideWordIdx """
 81  	
(Pdb) n
--Call--
> /usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner.py(323)__setattr__()
-> def __setattr__(self, attr, value):
(Pdb) n
> /usr/local/anaconda3/envs/a2/lib/python3.7/site-packages/behave/runner.py(324)__setattr__()
-> if attr[0] == "_":
(Pdb) c
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 195.176s
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44 0.001s
      Assertion Failed: 
      Not equal to tolerance rtol=1e-07, atol=0
      
      (shapes (3, 3), (3,) mismatch)
       x: array([[ 0.005489, -0.169955,  0.164466],
             [ 0.027444, -0.849774,  0.82233 ],
             [ 0.013722, -0.424887,  0.411165]])
       y: array([0.794886, 0.794886, 0.794886])



Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
14 steps passed, 1 failed, 0 skipped, 0 undefined
Took 3m15.179s
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 0.000s
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44 0.002s
      Assertion Failed: 
      Not equal to tolerance rtol=1e-07, atol=0
      
      (shapes (3, 3), (3,) mismatch)
       x: array([[ 0.005489, -0.169955,  0.164466],
             [ 0.027444, -0.849774,  0.82233 ],
             [ 0.013722, -0.424887,  0.411165]])
       y: array([0.794886, 0.794886, 0.794886])
      Captured stdout:
      (1.895613488056148, array([0.79488617, 0.79488617, 0.79488617]), array([[ 0.00548877, -0.16995477,  0.164466  ],
             [ 0.02744384, -0.84977386,  0.82233001],
             [ 0.01372192, -0.42488693,  0.41116501]]))



Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
14 steps passed, 1 failed, 0 skipped, 0 undefined
Took 0m0.005s
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 0.000s
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44 0.002s
      Assertion Failed: 
      Not equal to tolerance rtol=1e-07, atol=0
      
      (shapes (3, 3), (3,) mismatch)
       x: array([[ 0.005489, -0.169955,  0.164466],
             [ 0.027444, -0.849774,  0.82233 ],
             [ 0.013722, -0.424887,  0.411165]])
       y: array([0.794886, 0.794886, 0.794886])
      Captured stdout:
      ................ BEGIN ................
      (1.895613488056148, array([0.79488617, 0.79488617, 0.79488617]), array([[ 0.00548877, -0.16995477,  0.164466  ],
             [ 0.02744384, -0.84977386,  0.82233001],
             [ 0.01372192, -0.42488693,  0.41116501]]))



Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
14 steps passed, 1 failed, 0 skipped, 0 undefined
Took 0m0.004s
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 0.000s
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44 0.001s
      Assertion Failed: 
      Not equal to tolerance rtol=1e-07, atol=0
      
      (shapes (3, 3), (3,) mismatch)
       x: array([[ 0.005489, -0.169955,  0.164466],
             [ 0.027444, -0.849774,  0.82233 ],
             [ 0.013722, -0.424887,  0.411165]])
       y: array([0.794886, 0.794886, 0.794886])
      Captured stdout:
      ................ BEGIN ................
      (1.895613488056148, array([0.79488617, 0.79488617, 0.79488617]), array([[ 0.00548877, -0.16995477,  0.164466  ],
             [ 0.02744384, -0.84977386,  0.82233001],
             [ 0.01372192, -0.42488693,  0.41116501]]))
      (1.895613488056148, array([0.79488617, 0.79488617, 0.79488617]), array([0.79488617, 0.79488617, 0.79488617]))
      ~~~~~~~~~~~~~~~~  END  ~~~~~~~~~~~~~~~~



Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
14 steps passed, 1 failed, 0 skipped, 0 undefined
Took 0m0.004s
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                  # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                            # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                       # features/steps/word2vec_step.py:32 0.002s
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343] # features/steps/word2vec_step.py:44 0.002s
      Assertion Failed: 
      Not equal to tolerance rtol=1e-07, atol=0
      
      (shapes (3, 3), (3,) mismatch)
       x: array([[ 0.005489, -0.169955,  0.164466],
             [ 0.027444, -0.849774,  0.82233 ],
             [ 0.013722, -0.424887,  0.411165]])
       y: array([0.794886, 0.794886, 0.794886])
      Captured stdout:
      1.895613488056148
      [0.79488617 0.79488617 0.79488617]
      [[ 0.00548877 -0.16995477  0.164466  ]
       [ 0.02744384 -0.84977386  0.82233001]
       [ 0.01372192 -0.42488693  0.41116501]]
      ................ BEGIN ................
      (1.895613488056148, array([0.79488617, 0.79488617, 0.79488617]), array([[ 0.00548877, -0.16995477,  0.164466  ],
             [ 0.02744384, -0.84977386,  0.82233001],
             [ 0.01372192, -0.42488693,  0.41116501]]))
      (1.895613488056148, array([0.79488617, 0.79488617, 0.79488617]), array([0.79488617, 0.79488617, 0.79488617]))
      ~~~~~~~~~~~~~~~~  END  ~~~~~~~~~~~~~~~~



Failing scenarios:
  features/word2vec.feature:26  naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays

0 features passed, 1 failed, 0 skipped
4 scenarios passed, 1 failed, 0 skipped
14 steps passed, 1 failed, 0 skipped, 0 undefined
Took 0m0.006s
(a2) bash-3.2$ behave
Feature: Test Word2Vec Implementation # features/word2vec.feature:1

  Scenario Outline: sigmoid -- @1.1 float  # features/word2vec.feature:10
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 0.0              # features/steps/word2vec_step.py:10
    When it is applied to 0.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14
    Then 0.5 is returned                   # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.2 float  # features/word2vec.feature:11
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to 1.0              # features/steps/word2vec_step.py:10
    When it is applied to 1.0              # features/steps/word2vec_step.py:10 0.000s
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14
    Then 0.7310585786 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @1.3 float  # features/word2vec.feature:12
    Given a sigmoid function               # features/steps/word2vec_step.py:6
    Given a sigmoid function               # features/steps/word2vec_step.py:6 0.000s
    When it is applied to -1.0             # features/steps/word2vec_step.py:10
    When it is applied to -1.0             # features/steps/word2vec_step.py:10 0.000s
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14
    Then 0.2689414214 is returned          # features/steps/word2vec_step.py:14 0.000s

  Scenario Outline: sigmoid -- @2.1 ndarray            # features/word2vec.feature:16
    Given a sigmoid function                           # features/steps/word2vec_step.py:6
    Given a sigmoid function                           # features/steps/word2vec_step.py:6 0.000s
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19
    When it is applied to [-1, 0, 1]                   # features/steps/word2vec_step.py:19 0.000s
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23
    Then [0.2689414214, 0.5, 0.7310585786] is returned # features/steps/word2vec_step.py:23 0.001s

  Scenario Outline: naiveSoftmaxLossAndGradient -- @1.1 float and ndarrays                                                                                                                                                                                                                                                                               # features/word2vec.feature:26
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                                                                                                                                                                         # features/steps/word2vec_step.py:28
    Given a naiveSoftmaxLossAndGradient function                                                                                                                                                                                                                                                                                                         # features/steps/word2vec_step.py:28 0.000s
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                                                                                                                                                                    # features/steps/word2vec_step.py:32
    When it is applied to center_word_vec: [0.2, 1.0, 0.5], outside_word_idx: 1, outside_vectors: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]                                                                                                                                                                                                    # features/steps/word2vec_step.py:32 0.000s
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [[0.005488768308294598, -0.16995477103501588,  0.1644660027267213], [0.027443841541472988,  -0.8497738551750793,  0.8223300136336065], [0.013721920770736494, -0.42488692758753965, 0.41116500681680324]] # features/steps/word2vec_step.py:44
    Then it returns loss: 1.895613488056148, grad_center_vec: [0.7948861720921339, 0.7948861720921343, 0.7948861720921343], grad_outside_vecs: [[0.005488768308294598, -0.16995477103501588,  0.1644660027267213], [0.027443841541472988,  -0.8497738551750793,  0.8223300136336065], [0.013721920770736494, -0.42488692758753965, 0.41116500681680324]] # features/steps/word2vec_step.py:44 0.002s

1 feature passed, 0 failed, 0 skipped
5 scenarios passed, 0 failed, 0 skipped
15 steps passed, 0 failed, 0 skipped, 0 undefined
Took 0m0.004s
(a2) bash-3.2$ git status
fatal: not a git repository (or any of the parent directories): .git
(a2) bash-3.2$ pwd
/Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/a2
(a2) bash-3.2$ cd ..
(a2) bash-3.2$ ls
a1	a2
(a2) bash-3.2$ ls -a
.		..		.DS_Store	.env		a1		a2
(a2) bash-3.2$ git init
Initialized empty Git repository in /Users/kingcrawford/src/Stanford/Stanford_cs224n_Natural_Language_Processing_With_Deeplearning/assignments/.git/
(a2) bash-3.2$ git status
On branch master

No commits yet

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	.DS_Store
	.env/
	a1/
	a2/

nothing added to commit but untracked files present (use "git add" to track)
(a2) bash-3.2$ echo .env > .gitignore
(a2) bash-3.2$ git status
On branch master

No commits yet

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	.DS_Store
	.gitignore
	a1/
	a2/

nothing added to commit but untracked files present (use "git add" to track)
(a2) bash-3.2$ rm .DS_Store 
(a2) bash-3.2$ git add -A
(a2) bash-3.2$ git commit -m "assignment 1 and part of assignment 2"
[master (root-commit) 03980f3] assignment 1 and part of assignment 2
 Committer: King Crawford <kingcrawford@Kings-MacBook-Pro.local>
Your name and email address were configured automatically based
on your username and hostname. Please check that they are accurate.
You can suppress this message by setting them explicitly. Run the
following command and follow the instructions in your editor to edit
your configuration file:

    git config --global --edit

After doing this, you may fix the identity used for this commit with:

    git commit --amend --reset-author

 36 files changed, 7912 insertions(+)
 create mode 100644 .gitignore
 create mode 100644 a1/.ipynb_checkpoints/exploring_word_vectors-checkpoint.ipynb
 create mode 100644 a1/README.txt
 create mode 100644 a1/exploring_word_vectors.ipynb
 create mode 100644 a1/exploring_word_vectors.pdf
 create mode 100644 a1/imgs/inner_product.png
 create mode 100644 a1/imgs/svd.png
 create mode 100644 a1/imgs/test_plot.png
 create mode 100644 a1/src/active.py
 create mode 100644 a1/src/active.py~
 create mode 100644 a2/#*shell*#
 create mode 100644 a2/*shell*
 create mode 100644 a2/*shell*~
 create mode 120000 a2/.#*shell*
 create mode 100644 a2/__pycache__/word2vec.cpython-37.pyc
 create mode 100644 a2/a2.pdf
 create mode 100644 a2/collect_submission.sh
 create mode 100644 a2/env.yml
 create mode 100644 a2/features/environment.py
 create mode 100644 a2/features/steps/__pycache__/word2vec.cpython-37.pyc
 create mode 100644 a2/features/steps/word2vec_step.py
 create mode 100644 a2/features/steps/word2vec_step.py~
 create mode 100644 a2/features/word2vec.feature
 create mode 100755 a2/get_datasets.sh
 create mode 100644 a2/run.py
 create mode 100644 a2/sgd.py
 create mode 100644 a2/utils/.DS_Store
 create mode 100644 a2/utils/__init__.py
 create mode 100644 a2/utils/__pycache__/__init__.cpython-37.pyc
 create mode 100644 a2/utils/__pycache__/gradcheck.cpython-37.pyc
 create mode 100644 a2/utils/__pycache__/utils.cpython-37.pyc
 create mode 100644 a2/utils/gradcheck.py
 create mode 100644 a2/utils/treebank.py
 create mode 100644 a2/utils/utils.py
 create mode 100644 a2/word2vec.py
 create mode 100644 a2/word2vec.py~
(a2) bash-3.2$ git 